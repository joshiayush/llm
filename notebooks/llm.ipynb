{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshiayush/llm/blob/main/notebooks/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies Installation"
      ],
      "metadata": {
        "id": "AVCjgj9m9Czz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "QYNv2SHTizFY",
        "outputId": "b73e2d11-64f5-4601-b0b9-882d71b201de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GCr9jCncIzn"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "ZpMYW4ZUi2A1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = requests.get(\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "raw_text = raw_text.content.decode()"
      ],
      "metadata": {
        "id": "FPCOLM3Ri3px"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BYTE PAIR ENCODING\n",
        "\n",
        "**Advantages:**\n",
        "* Reduces vocab lenght.\n",
        "* Handles out-of-vocabulary words."
      ],
      "metadata": {
        "id": "16d4xSGhHa8F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjQkRNrncIzv"
      },
      "source": [
        "### Note\n",
        "\n",
        "__[BOS] (beginning of sequence):__ This token marks the start of a text. It\n",
        "signifies to the LLM where a piece of content begins.\n",
        "\n",
        "__[EOS] (end of sequence):__ This token is positioned at the end of a text,\n",
        "and is especially useful when concatenating multiple unrelated texts,\n",
        "similar to `<|endoftext|>`. For instance, when combining two different\n",
        "Wikipedia articles or books, the __[EOS]__ token indicates where one article\n",
        "ends and the next one begins.\n",
        "\n",
        "__[PAD] (padding):__ When training LLMs with batch sizes larger than one,\n",
        "the batch might contain texts of varying lengths. To ensure all texts have\n",
        "the same length, the shorter texts are extended or \"padded\" using the\n",
        "__[PAD]__ token, up to the length of the longest text in the batch.\n",
        "\n",
        "Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token for simplicity.\n",
        "\n",
        "The tokenizer used for GPT models also doesn't use an `<|unk|>` token for __out-of-vocabulary__ words. Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FCM8h6OtcIzv"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our BPE tokenizer."
      ],
      "metadata": {
        "id": "LJOCKX0bYAAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b2lVu9LDcIzv"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizer.encode(raw_text)"
      ],
      "metadata": {
        "id": "kKaUw7xC_yeD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9uQTx9ccIzw"
      },
      "source": [
        "### Implementing a Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tiktoken.core import Encoding\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "OuUtVphrDEPD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kL29TL1ycIzw"
      },
      "outputs": [],
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "\n",
        "    def __init__(self, txt: str, tokenizer: Encoding, max_length: int, stride: int):\n",
        "        self.x = list()\n",
        "        self.y = list()\n",
        "\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            x_chunk = token_ids[i : i + max_length]\n",
        "            y_chunk = token_ids[i + 1 : i + max_length + 1]\n",
        "            self.x.append(torch.tensor(x_chunk))\n",
        "            self.y.append(torch.tensor(y_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5oWeoQ7YcIzw"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(\n",
        "    txt: str,\n",
        "    batch_size: int = 4,\n",
        "    max_length: int = 256,\n",
        "    stride: int = 128,\n",
        "    shuffle: bool = True,\n",
        "    drop_last: bool = True,\n",
        "    num_workers: int = 0,\n",
        "):\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V-c9_F1-cIzx"
      },
      "outputs": [],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter = iter(dataloader)\n",
        "batch_1 = next(iter)"
      ],
      "metadata": {
        "id": "DNXVRovmH3ne"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at our first batch."
      ],
      "metadata": {
        "id": "uLmGN3KOH_JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_1)"
      ],
      "metadata": {
        "id": "PjSezMAbH7hW",
        "outputId": "e02c881b-c6f5-4cee-8507-cebc2693bd6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is the second batch."
      ],
      "metadata": {
        "id": "m0JxFFQGIDpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1IVbiwxbcIzx"
      },
      "outputs": [],
      "source": [
        "batch_2 = next(iter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_2)"
      ],
      "metadata": {
        "id": "kySMuMVXIG0F",
        "outputId": "75dc04f9-af4f-41d2-ceba-b48a046f87cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUA4j0iXcIzx"
      },
      "source": [
        "**CREATE TOKEN EMBEDDINGS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "WJxOe3VvIsPv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNF6L38kcIzx"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2RsT19JcIzx"
      },
      "outputs": [],
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR2Alx6acIzx",
        "outputId": "5fbc7ac3-7ea0-4d44-cdf8-84dfa9e07002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer.weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz1niAlEcIzx",
        "outputId": "e8833945-1564-4cc6-e64d-1a37b97e6b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(torch.tensor([3])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6H6mrdacIzx",
        "outputId": "0b62bc09-19de-4a72-a53c-d847ad2a40ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gSbGPtfcIzx"
      },
      "source": [
        "**POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySruk-a0cIzx"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q3NE-ZgcIzx"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJFeJeWcIzy",
        "outputId": "558b3d88-8754-461e-8740-fdb40978b583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTN_nXGHcIzy",
        "outputId": "42ac3d47-142d-4fd5-98f5-bcd9e7c225b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkvSfpjHcIzy"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ-6Ye7GcIzy",
        "outputId": "ee1d1f31-1d1f-4008-f8bc-c6dccfb20558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28JONs9xcIzy",
        "outputId": "21028850-97b7-4478-e036-7b0971381014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceI4clyhcIzy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}